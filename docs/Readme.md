# Idaho State Univeristy Artificial Intelligence -- Augmented Reality Prosthetic Measurement Research

This project leverages Unity's built-in machine learning (ML) agents, which use reinforcement learning, to design a prosthetic arm tailored to individual users. The proposed workflow involves integrating real-time data from a physical prosthetic device with ML agents in a Unity environment. The ML agents adjust the scale and relative positioning of the prosthetic components based on user-specific inputs.

The process includes creating a Unity scene where a virtual prosthetic arm performs tasks in sync with the user completing the same tasks in physical reality. Over time, the ML agents refine the virtual arm's precision and efficiency until it mirrors the userâ€™s movements. The final output is a personalized prosthetic arm design, ready for fabrication using tools like SolidWorks.

Key considerations include optimizing the efficiency of the ML agents to minimize the time required for user interaction during the calibration process.

## Unity Scene: ARAI_Hub

To use the Unity Scene please download the files and copy the ARAI_Hub Project onto your disk. To control the prosthetic a Thalmic Lab's EMG Sensor is necessary, however the framework for the Unity Scene which uses feature engineering, and the ML-Agents are still present.
